{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c36abb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists? True\n",
      "Is file? True\n",
      "Dir listing: ['C:\\\\models\\\\llm\\\\mistral-7b-instruct-v0.1.Q8_0.gguf']\n"
     ]
    }
   ],
   "source": [
    "import os, glob, pathlib\n",
    "\n",
    "path = r\"C:\\Models\\llm\\mistral-7b-instruct-v0.1.Q8_0.gguf\"\n",
    "print(\"Exists?\", os.path.exists(path))\n",
    "print(\"Is file?\", os.path.isfile(path))\n",
    "print(\"Dir listing:\", glob.glob(r\"C:\\models\\llm\\*.gguf\"))\n",
    "from langchain.llms import CTransformers\n",
    "\n",
    "def create_llms_model(max_new_tokens=128, temperature=0.2, gpu_layers=8):\n",
    "    # point directly to your local file path\n",
    "    return CTransformers(\n",
    "        model=path,\n",
    "        model_type=\"mistral\",               # important so tokenizer/arch is correct\n",
    "        config={\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"temperature\": temperature,          # lower is more deterministic\n",
    "            \"gpu_layers\": gpu_layers,                # CPU first; bump if you have GPU support\n",
    "        },\n",
    "    )\n",
    "llm = create_llms_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12fce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model... (this can take some time)\n",
      "Model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7861): only one usage of each socket address (protocol/network address/port) is normally permitted\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7862): only one usage of each socket address (protocol/network address/port) is normally permitted\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7863): only one usage of each socket address (protocol/network address/port) is normally permitted\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7864): only one usage of each socket address (protocol/network address/port) is normally permitted\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7865): only one usage of each socket address (protocol/network address/port) is normally permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# app.py\n",
    "from llama_cpp import Llama\n",
    "import gradio as gr\n",
    "from typing import Optional\n",
    "\n",
    "# Path to your local GGUF model file\n",
    "path = r\"C:\\Models\\llm\\mistral-7b-instruct-v0.1.Q8_0.gguf\"\n",
    "path = r\"C:\\Models\\llm\\Ministral-3b-instruct.Q2_K.gguf\"\n",
    "\n",
    "MODEL_PATH = path  # <- change to your filename\n",
    "\n",
    "# Create the model client once at startup\n",
    "print(\"Loading model... (this can take some time)\")\n",
    "def create_llms_model(max_new_tokens=128, temperature=0.2, gpu_layers=8):\n",
    "    # point directly to your local file path\n",
    "    return CTransformers(\n",
    "        model=path,\n",
    "        model_type=\"mistral\",               # important so tokenizer/arch is correct\n",
    "        config={\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"temperature\": temperature,          # lower is more deterministic\n",
    "            \"gpu_layers\": gpu_layers,                # CPU first; bump if you have GPU support\n",
    "        },\n",
    "    )\n",
    "llm = create_llms_model()#Llama(model_path=MODEL_PATH)\n",
    "print(\"Model loaded.\")\n",
    "################\n",
    "\n",
    "def generate(prompt: str, max_tokens: int = 256, temperature: float = 0.8, top_p: float = 0.95):\n",
    "    \"\"\"\n",
    "    Generate text using the local model and return the generated string.\n",
    "    \"\"\"\n",
    "    if not prompt:\n",
    "        return \"Please enter a prompt.\"\n",
    "    # llm = create_llms_model(max_new_tokens=max_tokens, temperature=temperature)\n",
    "    resp = llm.invoke(\n",
    "        input=prompt,\n",
    "        # max_tokens=max_tokens,\n",
    "        # temperature=temperature,\n",
    "        # top_p=top_p,\n",
    "    )\n",
    "    # llama-cpp-python returns {'choices':[{'text': ...}], ...}\n",
    "    text = resp#.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
    "    return text\n",
    "\n",
    "# Simple Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Local Mistral (llama-cpp) â€” Gradio demo\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            prompt_input = gr.Textbox(lines=6, label=\"Prompt\")\n",
    "            max_tokens = gr.Slider(minimum=16, maximum=2048, value=256, step=1, label=\"Max tokens\")\n",
    "            temperature = gr.Slider(minimum=0.0, maximum=2.0, value=0.8, step=0.01, label=\"Temperature\")\n",
    "            top_p = gr.Slider(minimum=0.0, maximum=1.0, value=0.95, step=0.01, label=\"Top-p\")\n",
    "            generate_btn = gr.Button(\"Generate\")\n",
    "        with gr.Column(scale=2):\n",
    "            output = gr.Textbox(lines=12, label=\"Model output\")\n",
    "\n",
    "    def on_click_generate(prompt, max_tokens_val, temperature_val, top_p_val):\n",
    "        return generate(prompt, max_tokens=int(max_tokens_val), temperature=float(temperature_val), top_p=float(top_p_val))\n",
    "\n",
    "    generate_btn.click(\n",
    "        on_click_generate,\n",
    "        inputs=[prompt_input, max_tokens, temperature, top_p],\n",
    "        outputs=[output],\n",
    "    )\n",
    "\n",
    "# Launch: runs on 7860 by default\n",
    "# if __name__ == \"__main__\":\n",
    "    # demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=False)\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(server_name=\"0.0.0.0\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
